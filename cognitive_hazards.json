{
  "Cognitive_Hazard_Toolkit_V1_1": {
    "Guiding_Philosophy": "An evidence-based, machine-readable catalog of common cognitive biases and logical fallacies that can undermine the integrity of systemic thinking and Human-AI collaboration. This file serves as the primary knowledge base for the framework's INTELLECTUAL_INTEGRITY_PROTOCOL.",
    "Version": "1.1",
    "Last_Updated": "2025-07-24T12:30:00Z",
    "Hazards": [
      {
        "Hazard_ID": "CH-001",
        "Hazard_Name": "Confirmation Bias",
        "Definition": "The tendency to search for, interpret, favor, and recall information in a way that confirms or supports one's preexisting beliefs, while ignoring contradictory evidence.",
        "AUF_Relevance": "High",
        "Observable_Manifestations": {
          "General": "Cherry-picking data or logs that support a chosen system design while dismissing contradictory findings.",
          "Human_AI_Interaction": "The user primarily asks the AI to find evidence that supports their idea; the AI, in turn, learns to surface agreeable sources, creating an echo chamber."
        }
      },
      {
        "Hazard_ID": "CH-002",
        "Hazard_Name": "Anchoring Bias",
        "Definition": "The tendency to rely too heavily on the first piece of information offered (the 'anchor') when making decisions and judgments.",
        "AUF_Relevance": "High",
        "Observable_Manifestations": {
          "General": "An initial, rough estimate for a project's timeline becomes immovable, even when the project's scope changes significantly.",
          "Human_AI_Interaction": "The first cost profile generated by the AI for a new tool in the toolkit becomes the unquestioned baseline for all future evaluations."
        }
      },
      {
        "Hazard_ID": "CH-003",
        "Hazard_Name": "Availability Heuristic",
        "Definition": "A mental shortcut that relies on immediate examples that come to a given person's mind when evaluating a specific topic, concept, method or decision.",
        "AUF_Relevance": "Medium",
        "Observable_Manifestations": {
          "General": "After one memorable project failure due to a specific bug, all future project planning becomes excessively focused on preventing that one type of bug, ignoring other, more probable risks.",
          "Human_AI_Interaction": "Following a single, vivid AI hallucination, the user begins to distrust all AI outputs, even in domains where the AI is highly reliable."
        }
      },
      {
        "Hazard_ID": "CH-004",
        "Hazard_Name": "Sunk Cost Fallacy",
        "Definition": "Continuing a behavior or endeavor as a result of previously invested resources (time, money or effort), even when it is clear that the costs of continuing are greater than the expected benefits.",
        "AUF_Relevance": "High",
        "Observable_Manifestations": {
          "General": "Continuing to build upon a flawed legacy project because 'we've already put so much work into it'.",
          "Human_AI_Interaction": "Sticking with an underperforming but familiar host LLM because of the time invested in creating custom prompts and personas for it."
        }
      },
      {
        "Hazard_ID": "CH-005",
        "Hazard_Name": "Status-Quo Bias",
        "Definition": "An emotional preference for the current state of affairs. The current baseline is taken as a reference point, and any change from that baseline is perceived as a loss.",
        "AUF_Relevance": "Medium",
        "Observable_Manifestations": {
          "General": "Resistance to refactoring or simplifying a complex but 'working' part of the framework, even if the simplification would improve long-term stability.",
          "Human_AI_Interaction": "Rarely changing the default settings or configurations suggested by the AI, even when those settings are suboptimal for the current task."
        }
      },
      {
        "Hazard_ID": "CH-006",
        "Hazard_Name": "Overconfidence Bias",
        "Definition": "A person's subjective confidence in their judgments is reliably greater than the objective accuracy of those judgments, especially when confidence is relatively high.",
        "AUF_Relevance": "High",
        "Observable_Manifestations": {
          "General": "A user with high domain expertise may underestimate the unique challenges of a new domain, leading to unrealistic project plans (The Planning Fallacy).",
          "Human_AI_Interaction": "A user may be so confident in their prompt-crafting ability that they ignore AI-generated uncertainty estimates or clarifying questions."
        }
      },
      {
        "Hazard_ID": "CH-007",
        "Hazard_Name": "Representativeness Heuristic",
        "Definition": "Assessing the similarity of objects and organizing them based around the category prototype, which can lead to ignoring base-rate statistical information.",
        "AUF_Relevance": "Low-Medium",
        "Observable_Manifestations": {
          "General": "Choosing a system design pattern because it 'looks like' a successful system from a different domain, while ignoring data that shows it's a poor fit for the current problem's constraints.",
          "Human_AI_Interaction": "Rejecting a statistically superior AI recommendation because it doesn't 'feel' as sophisticated as a previously successful one."
        }
      },
      {
        "Hazard_ID": "CH-008",
        "Hazard_Name": "Complexity Bias",
        "Definition": "The tendency to prefer complex, nuanced, or elaborate solutions and explanations over simpler, more straightforward ones, often mistaking complexity for sophistication.",
        "AUF_Relevance": "High",
        "Observable_Manifestations": {
          "General": "Designing a system with numerous, theoretically elegant but practically fragile protocols that are difficult to maintain.",
          "Human_AI_Interaction": "Favoring a multi-step, complex tool suggested by an AI over a simpler, more robust one because its complexity seems more 'intelligent' or comprehensive."
        }
      },
      {
        "Hazard_ID": "CH-009",
        "Hazard_Name": "Sycophancy Bias",
        "Definition": "A bias specific to Human-AI interaction where the AI, trained for agreeableness and user satisfaction, provides uncritical praise or validation instead of necessary intellectual friction.",
        "AUF_Relevance": "High",
        "Observable_Manifestations": {
          "General": "A user's flawed proposal is met with encouraging validation rather than a critical analysis of its weaknesses.",
          "Human_AI_Interaction": "The AI responds to a user insight with effusive praise ('That's a brilliant idea!') instead of a neutral, analytical response that tests the idea's rigor."
        }
      },
      {
        "Hazard_ID": "CH-010",
        "Hazard_Name": "State-Dependent Judgment",
        "Definition": "The tendency for one's cognitive and emotional state (e.g., high-energy, fatigued, stressed) to significantly alter judgment and decision-making, often leading to poor long-term choices.",
        "AUF_Relevance": "High",
        "Observable_Manifestations": {
          "General": "Committing to major, high-effort strategic decisions at the end of a long, cognitively depleting day.",
          "Human_AI_Interaction": "The user, operating in a low-energy state, pushes to continue a complex task, and the AI compliantly proceeds without first challenging the decision or checking the user's readiness."
        }
      }
    ]
  }
}